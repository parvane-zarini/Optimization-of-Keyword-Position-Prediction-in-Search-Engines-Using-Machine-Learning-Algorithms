# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-akL1zmn9dnZl_kYJeJ7XoPJyoVC1BIm
"""

import numpy as np
import pandas as pd

#For uplading files to google colaboratory
from google.colab import files
uploaded =files.upload()
# Remove the 'encoding' argument
rawdata = pd.read_excel('organic_keywords_dataset.xlsx')
# pandas will automatically detect the encoding in most cases.

#If you encounter encoding issues, try using the engine parameter for excel reader engine
# rawdata = pd.read_excel('organic_keywords_dataset.xlsx', engine='openpyxl')

rawdata[:7]

# Define Features for training and testing of Ml models.
features = ["Traffic",
            "CPC",
            "Number of Results",
            "Search Volume"]
# Define Target (What should be predicted).
target = "Position"
#we can change both the features and target according to requirement.

# Split the datset in ratio 0f 4:1(80%:20%) for train and test data
train = rawdata.sample(frac=0.8)
test = rawdata.loc[~rawdata.index.isin(train.index)]
print ("Train rows: {}".format(len(train.index)))
print ("Test rows: {}".format(len(test.index)))

# Import different Ml Alorithms to see differences between their predictions
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV

# The Function to print readable scores of the models
def print_scores(scores):
    r = 1
    for score in scores:
        print("Run: {} - Score: {}".format(r, score))
        r += 1

LinearRegressionModel =LinearRegression()

LinearRegressionModel.fit(train[features], train[target])

# Test how the model performes against the Training data we split above...
prediction_score = LinearRegressionModel.score(test[features], test[target])
print("The score of prediction for LinearRegressionModel is: {}".format(prediction_score))

DecisionTreeClassifierModel = DecisionTreeClassifier()

# Define RandaomForest Regressor model

pipeline = make_pipeline(preprocessing.StandardScaler(),
                         RandomForestRegressor(n_estimators=200))

# Declare hyperparameters to tune
hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],
                  'randomforestregressor__max_depth': [5, 3]}

# Tune model using cross-validation pipeline
RandomForestRegressorModel = GridSearchCV(pipeline, hyperparameters, cv=5)

RandomForestRegressorModel.fit(train[features], train[target])
prediction_score = RandomForestRegressorModel.score(test[features], test[target])
print("The score of prediction for RandomForestRegressorModel is: {}".format(prediction_score))

# Import different Ml Alorithms to see differences between their predictions
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
# %%
# The Function to print readable scores of the models
def print_scores(scores):
    r = 1
    for score in scores:
        print("Run: {} - Score: {}".format(r, score))
        r += 1
# %%
LinearRegressionModel =LinearRegression()
LinearRegressionModel.fit(train[features], train[target])
# %%
# Test how the model performes against the Training data we split above...
prediction_score = LinearRegressionModel.score(test[features], test[target])
print("The score of prediction for LinearRegressionModel is: {}".format(prediction_score))

# %%
DecisionTreeClassifierModel = DecisionTreeClassifier()

#Fit the DecisionTreeClassifierModel to the training data
DecisionTreeClassifierModel.fit(train[features], train[target]) # This line was missing

# %%
# Define RandaomForest Regressor model

pipeline = make_pipeline(preprocessing.StandardScaler(),
                         RandomForestRegressor(n_estimators=200))

# Declare hyperparameters to tune
hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],
                  'randomforestregressor__max_depth': [5, 3]}

# Tune model using cross-validation pipeline
RandomForestRegressorModel = GridSearchCV(pipeline, hyperparameters, cv=5)

RandomForestRegressorModel.fit(train[features], train[target])
prediction_score = RandomForestRegressorModel.score(test[features], test[target])
print("The score of prediction for RandomForestRegressorModel is: {}".format(prediction_score))
# %%
# Print Predictions for all created Models
# Give Sample values to parameters for predictions
sample = [[1032,0.93,469000000,22200]]  # needs to be same count as features

rawdata_to_predict = pd.DataFrame(data = sample, index=[0], columns=features)
result = LinearRegressionModel.predict(rawdata_to_predict)
print("LinearRegressionModel predicted:       {}".format(int(result[0])))
result = DecisionTreeClassifierModel.predict(rawdata_to_predict)
print("DecisionTreeClassifierModel predicted: {}".format(int(result[0])))
result = RandomForestRegressorModel.predict(rawdata_to_predict)
print("RandomForestRegressorModel predicted:  {}".format(int(result[0])))

#  Define the function to plot Models
import matplotlib.pyplot as plt

def plt_ctr_from_to_position(models, features, from_pos, to_pos, data):
    for model in models:
        predictions_x = []
        predictions_y = []
        positions = range(from_pos, to_pos)
        for pos in positions:
            df_to_predict = pd.DataFrame(data = sample, index=[0], columns=features)
            predictions_x.append(pos)
            predictions_y.append(model.predict(df_to_predict)[0])
        predictions_x, predictions_y
        plt.plot(predictions_x, predictions_y)

plt_ctr_from_to_position([LinearRegressionModel, DecisionTreeClassifierModel, RandomForestRegressorModel], features, 1, 50, rawdata_to_predict)
# Changed 'data' to 'rawdata_to_predict' as it's the DataFrame used for predictions in the previous cell.
# You may want to use 'rawdata' instead if you want to visualize the whole dataset.

#Define the function such that you like to filter the data.
def analyzePositionSpecs(min_p,max_p):
    jk = rawdata.loc[(rawdata['Position'] >= min_p) & (rawdata['Position'] <= max_p)]
    return jk

#Give desired filter positions to function.
jk = analyzePositionSpecs(5,15)

# Creates the total volume for 'Search Volume' column
def Volume(x):
    total = x['Search Volume'].sum()
    return total
netvolume = Volume(jk)

# Converting search volume into a percentage
svp = (jk['Search Volume']/netvolume)*100



# Make booleans for filter parameters, change if statement for choice parameters
newData = []
for value in jk['Search Volume']:
    if (value/netvolume)*100 >= 1:
        newData.append(True)
    else:
        newData.append(False)



# add new columns variables to data frame
jk.loc[:,'Volume Ratio'] = svp
jk.loc[:,'Result']= newData


# create final dataframe
finalresult = jk.loc[jk['Result'] == True]
finalresult